{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edited by Krish Naik \n",
    "    traget is having the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MSZoning'].value_counts()\n",
    "df.isnull().sum() # view nb missing values by cloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False) # cmap='viridis'\n",
    "df.shape; df.info() # see df RAM usage if is_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())\n",
    "df.drop(['Alley'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtCond']= df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "df['BsmtQual']= df['BsmtQual'].fillna(df['BsmtQuall'].mode()[0])\n",
    "\n",
    "df['Fireplace'] = df['Fireplace'].fillna(df['Fireplace'].mode()[0])\n",
    "df['GarageType'] = df['GarageType'].fillna(df['GarageType'].mode()[0])\n",
    "\n",
    "df.drop(['GarageYrBlt'], axis=1, inplace=True)\n",
    "\n",
    "df['GarageFinish'] = df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "df['GarageQual'] = df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageCond'] = df['GarageCond'].fillna(df['GarageCond'].mode()[0])\n",
    "\n",
    "df.drop(['PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)\n",
    "df['MSZoning'].values_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Id'], axis=1, inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MasVnrType'] = df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "df['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0])\n",
    "\n",
    "# see null values by field on datasets\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='coolwarm')\n",
    "df['BsmtExposure'] = df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.shape; test_df.head()\n",
    "\n",
    "test_df.drop(['Alley'], axis=1, inplace=True)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'Neighboors', 'Condition2',\n",
    "    'BldgType', 'Condition1', 'HouseStyle', 'SaleType', 'SaleCondition', 'ExterCond', 'ExterQual',\n",
    "    'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n",
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catergory_onehot_multcols(multcolumns):\n",
    "    df_final = final_df\n",
    "    i = 0\n",
    "    for fields in multcolumns:\n",
    "        print(fields)\n",
    "        df1 = pd.get_dummies(final_df[fields], drop_first=True)\n",
    "        \n",
    "        final_df = df.drop([fields], axis=1, inplace=True)\n",
    "        if i == 0:\n",
    "            df_final = df1.copy()\n",
    "        else:\n",
    "            df_final = pd.concat([df_final, df1], axis=1)\n",
    "        i += 1\n",
    "    df_final = pd.concat([final_df, df_final], axis = 1)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('formulatedtest.csv')\n",
    "test_df.head()\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df = pd.concat([df, test_df], axis=0)\n",
    "\n",
    "# remove duplicated columns\n",
    "final_df = final_df.loc[:, ~final_df.columns.duplicated()]\n",
    "\n",
    "final_df.shape\n",
    "final_df = catergory_onehot_multcols(columns)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train = final_df.iloc[:1422, :]\n",
    "df_Train = final_df.iloc[1422: , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test = None\n",
    "df_Test.drop(['SalePrice'], axis=1, inplace=True)\n",
    "df_Train['SalePrice'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train.drop(['Wall'], axis=1, inplace=True)\n",
    "df_Train.corr()['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_Train.drop(['SalePrice'], axis=1)\n",
    "y_train = df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "classifier = xgboost.XGBRegressor() # not hyperized yet\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgboost.XGBRegressor() # compare with RandomForestRegressor() next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5 ,10, 15]\n",
    "booster = ['gbtree', 'gblinear']\n",
    "learning_rate = [0.05, 0.1, 0.15, 0.20]\n",
    "min_child_wieght = [1, 2, 3, 4]\n",
    "base_score = [0.25, 0.5, 0.75, 1]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'booster': booster,\n",
    "    'learning_rate': learning_rate,\n",
    "    'min_child_wieght': min_child_wieght, \n",
    "    'base_score': base_score\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Set up Random search with 4-fold cross validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RandomizedSearchCV\n",
    "\n",
    "random_cv = RandomizedSearchCV(estimator=regressor, param_distributions=hyperparameter_grid, random_state = 42,\n",
    "    cv = 5, n_iter = 50, scoring='neg_mean_absolute_error', n_jobs=4, verbose = 5, return_train_score = True)\n",
    "\n",
    "random_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = random_cv.best_estimator_ # shows the param_values required\n",
    "\"\"\"hyperparameter-optimization of the model {regressor}\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bytree=1, silent=True, max_depth=3,\n",
    "    learning_rate=0.05, min_child_wieght=1, missing=None, n_estimators=900, n_jobs=1, nthread=None, seed=None, \n",
    "    objective='reg:linear', random_state=0, reg_aplha=0, reg_lambda=2, scale_pos_weight=1, gama=0, subsample=1)\n",
    "\"\"\"\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    "y_pred = regressor.predict(df_Test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = regressor.predict(df_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample submission file and Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(y_pred)\n",
    "sub_df = pd.read_csv('sample_submission.csv')\n",
    "datasets = pd.concat([sub_df['Id'], pred], axis=1)\n",
    "datasets.columns = ['Id', 'SalePrice']\n",
    "datasets.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = pd.DataFrame(rf_pred) # This is a random_forest\n",
    "sub_df = pd.read_csv('sample_submission.csv')\n",
    "datasets = pd.concat([sub_df['Id'], rf_pred], axis=1)\n",
    "datasets.columns = ['Id', 'SalePrice']\n",
    "datasets.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typos\n",
    "    use correlation\n",
    "    model.fit(df_Train + df_Test) \n",
    "    remove non-relevant data_fields out_subject of the thème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_Train.corr()\n",
    "corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0], ), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i, j] >= 0.9:\n",
    "            if columns[j]: columns[j] = False\n",
    "\n",
    "selected_columns = df_Train.columns[columns]\n",
    "data = df_Train[selected_columns]\n",
    "print(columns)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
